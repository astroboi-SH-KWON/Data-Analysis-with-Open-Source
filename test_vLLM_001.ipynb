{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jaehwachung/Data-Analysis-with-Open-Source/blob/main/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D_13%EA%B0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGglZIa2Bt0V"
   },
   "outputs": [],
   "source": [
    "# # LLM 처리를 위한 VLLM 설치 (오래걸리는 작업이므로 미리 실행!)\n",
    "# !pip install vllm\n",
    "# # 필요 시 세션 재시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVANXVuyxoEG"
   },
   "outputs": [],
   "source": [
    "# # 한글 처리를 위한 matplotlib 설정 (1)\n",
    "#\n",
    "# !sudo apt-get install -y fonts-nanum\n",
    "# !sudo fc-cache –fv\n",
    "# !rm ~/.cache/matplotlib -rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trq1ANRexrPL"
   },
   "source": [
    "- 런타임 -> 세션 다시 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDTgAuhmxqWq"
   },
   "outputs": [],
   "source": [
    "# 한글 처리를 위한 matplotlib 설정 (2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='NanumBarunGothic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hT-GVz0r5GY8"
   },
   "source": [
    "## 13-12 VLLM 라이브러리 설치 및 LLM 모델 로드\n",
    "\n",
    "주의\n",
    "- 런타임 유형 : GPU\n",
    "- 라이브러리 설치 및 모델 다운로드에 수 분(>6분)의 시간이 소요됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T02:20:52.281326Z",
     "start_time": "2025-12-30T01:59:17.051259Z"
    },
    "id": "kLVH4M9R_wC5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankus/miniconda3/envs/test_vLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-30 15:32:07,941\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-30 15:32:08 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 12-30 15:32:09 config.py:2444] Downcasting torch.float32 to torch.float16.\n",
      "INFO 12-30 15:32:16 config.py:549] This model supports multiple tasks: {'generate', 'score', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 12-30 15:32:16 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct', speculative_config=None, tokenizer='LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 12-30 15:32:18 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 12-30 15:32:18 cuda.py:226] Using XFormers backend.\n",
      "INFO 12-30 15:32:18 model_runner.py:1110] Starting to load model LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct...\n",
      "INFO 12-30 15:32:19 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 12-30 15:32:19 weight_utils.py:270] Time spent downloading weights for LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct: 0.556648 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.01it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-30 15:32:22 model_runner.py:1115] Loading model weights took 4.5145 GB\n",
      "INFO 12-30 15:32:24 worker.py:267] Memory profiling takes 1.72 seconds\n",
      "INFO 12-30 15:32:24 worker.py:267] the current vLLM instance can use total_gpu_memory (10.75GiB) x gpu_memory_utilization (0.80) = 8.60GiB\n",
      "INFO 12-30 15:32:24 worker.py:267] model weights take 4.51GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.01GiB; the rest of the memory reserved for KV Cache is 3.02GiB.\n",
      "INFO 12-30 15:32:24 executor_base.py:111] # cuda blocks: 2635, # CPU blocks: 3495\n",
      "INFO 12-30 15:32:24 executor_base.py:116] Maximum concurrency for 16000 tokens per request: 2.63x\n",
      "INFO 12-30 15:32:27 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-30 15:32:43 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.21 GiB\n",
      "INFO 12-30 15:32:43 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 20.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "## LLM 모델 로드 및 설정\n",
    "llm = LLM(\n",
    "    model=\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"half\",\n",
    "    gpu_memory_utilization=0.8, # # 0.6,  # # 메모리 사용량 0.8(최대 80%)까지 사용 설정\n",
    "    max_model_len=16000,  # # 32000,  # # 모델에서 생성 가능한 단어의 개수\n",
    ")\n",
    "\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_id = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "\n",
    "# # 토크나이저 및 모델 로드 (vLLM 대신)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.float32, # CPU 환경이므로 float32 권장  # # torch_dtype=torch.bfloat16,  # CPU가 지원한다면 메모리 사용량을 절반으로\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     device_map=\"cpu\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# prompt = \"안녕하세요, EXAONE 모델님! 자기소개 부탁드려요.\"\n",
    "# # EXAONE-3.5 권장 프롬프트 포맷 적용 (필요시)\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9dIjmw1-n1T"
   },
   "source": [
    "## 13-13 LLM 샘플링 파라미터 설정 및 프롬프트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WygrPK_V_xl5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.52s/it, est. speed input: 16.19 toks/s, output: 83.50 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "생성된 텍스트: 대한민국의 수도는 **서울**입니다. 서울은 역사와 현대가 공존하는 매력적인 도시로, 다양한 관광 명소와 즐길거리를 제공합니다:\n",
      "\n",
      "1. **경복궁**: 조선 시대의 궁궐로, 아름다운 건축물과 정원을 감상할 수 있습니다. 특히 봄에는 벚꽃이 만발해 더욱 아름답습니다.\n",
      "   \n",
      "2. **인사동**: 전통 문화와 예술이 살아 숨 쉬는 지역으로, 전통 공예품 가게와 갤러리가 많아 문화 체험에 좋습니다.\n",
      "\n",
      "3. **남산 N서울타워**: 서울 시내 전경을 한눈에 볼 수 있는 전망대입니다. 야간에는 조명이 아름다워 사진 촬영하기 좋습니다.\n",
      "\n",
      "4. **한강공원**: 한강을 따라 펼쳐진 공원에서 산책하거나 자전거를 타며 여유로운 시간을 보내실 수 있습니다. 특히 가을이면 단풍 구경도 즐길 만합니다.\n",
      "\n",
      "5. **명동 및 동대문 디자인 플라자 (DDP)**: 쇼핑과 현대 예술 작품 감상을 동시에 즐길 수 있는 곳입니다. 특히 DDP는 독특한 건축 디자인으로 유명합니다.\n",
      "\n",
      "6. **국립중앙박물관 및 국립현대미술관**: 한국의 역사와 현대 미술 작품을 한눈에 볼 수 있어 문화적 경험이 풍부합니다.\n",
      "\n",
      "이러한 장소들은 서울 여행의 핵심 포인트로, 다양한 취향과 관심사에 맞게 선택하시면 좋겠습니다!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams ## SamplingParams 클래스 임포트\n",
    "\n",
    "## 샘플링 파라미터 설정\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.3, ## (사실적 0.0, 창의적 1.0)생성 텍스트의 다양성 조절\n",
    "    top_p=1.0, ## top_p 누적 확률 내에서 토큰 샘플링\n",
    "    max_tokens=512, ## 생성될 최대 토큰 수\n",
    "    frequency_penalty=0.5 ## 자주 나타나는 토큰에 대한 패널티\n",
    ")\n",
    "\n",
    "def format_prompt(user_input: str) -> str:\n",
    "    ## 프롬프트 형식화 함수\n",
    "    messages = [\n",
    "        # 시스템 메시지 추가  # # 시스템 메시지는 모델 마다 고정되어 있음. 모델 페이지에서 확인(https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct)\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "        # 사용자 메시지 추가\n",
    "        {\"role\": \"user\", \"content\": user_input },\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "## 프롬프트 생성\n",
    "prompt = format_prompt(\"대한민국의 수도는 어디인가요? 수도에 여행하러간다면 어떤 즐길거리가 있을까요?\")\n",
    "\n",
    "## LLM을 사용하여 텍스트 생성\n",
    "# chat 함수를 이용 (프롬프트와 샘플링 파라미터)\n",
    "outputs = llm.chat(prompt, sampling_params)\n",
    "\n",
    "## 생성된 텍스트 출력\n",
    "print(\"\\n생성된 텍스트:\", outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOuwGZ_r-9HV"
   },
   "source": [
    "## 13-14 LLM 기반 분류 프롬프트 및 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Eea5BV77_0Zp"
   },
   "outputs": [],
   "source": [
    "## 분류 프롬프트 형식화 함수\n",
    "def format_classifier_prompt(board_title, title) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"주어진 글의 제목을 분류하는 AI 모델입니다.\n",
    "분류 클래스는 학사/전공, 학생활동, 외부정보로 나뉘어집니다.\n",
    "- 학사/전공: 교재, 과제, 시험 등 학업 및 전공 관련 내용\n",
    "- 학생활동: 동아리, 스터디, 모임 등 학생들의 자발적 활동\n",
    "- 외부정보: 취업, 공모전, 행사 등 외부 정보\"\"\"},  # # 분류 작업 지정\n",
    "        {\"role\": \"user\", \"content\": \"\"\"예시)\n",
    "전공명: 컴퓨터학과, 제목: 프로그래밍 과제 질문 -> 학사/전공\n",
    "전공명: 컴퓨터학과, 제목: 알고리즘 스터디 모집 -> 학생활동\n",
    "전공명: 컴퓨터학과, 제목: IT 취업 박람회 -> 외부정보\"\"\"},  # # 분류 작업 예시\n",
    "        {\"role\": \"user\", \"content\": f\"전공명: {board_title}\\n제목: {title}\"}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "## 분류 수행 함수\n",
    "def classify(major, title):\n",
    "    prompt = format_classifier_prompt(major, title) ## 분류 프롬프트 생성\n",
    "    outputs = llm.chat([prompt], sampling_params) ## LLM으로 분류 실행\n",
    "    return outputs[0].outputs[0].text ## 분류 결과 텍스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tNWLOItH_3Sa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 188.58 toks/s, output: 75.82 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'제목: \"딥러닝 개발 중에 질문 있습니다.\"\\n\\n**분류:** 학사/전공  \\n**이유:** 이 제목은 특정 전공인 컴퓨터학과에서 딥러닝이라는 학문 분야 내에서의 기술적 질문을 나타내므로, 학업 및 전공 관련 내용에 해당합니다. 따라서 **학사/전공** 클래스로 분류됩니다.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('컴퓨터학과', '딥러닝 개발 중에 질문 있습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8yho1k1yrA0C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 211.01 toks/s, output: 75.28 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**분류 결과:** 학생활동  \\n**설명:** 제목이 컴퓨터학과 학생들의 자발적인 동아리 활동을 소개하고 있으며, 신규 단원을 모집하고 특별한 혜택을 제공한다는 점에서 학생들의 학문 외적인 활동과 관련이 깊습니다. 따라서 **학생활동** 분류에 해당합니다.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('컴퓨터학과', '연극 동아리 회원 모집!! 신규 단원 혜택! AI로 배우는 연극!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMFn08KB_G36"
   },
   "source": [
    "## 13-15 분류 결과 파싱 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3TpGbVJd_4yg"
   },
   "outputs": [],
   "source": [
    "def get_class(analysis_result):\n",
    "    ## 분석 결과에서 분류 클래스 추출\n",
    "    classes = ['학사/전공', '학생활동', '외부정보', '기타']\n",
    "    classes_index = [analysis_result.find(cls) for cls in classes]\n",
    "\n",
    "    min_index = float('inf')\n",
    "    min_class = '기타'\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        current_index = classes_index[i]\n",
    "        if current_index != -1 and current_index < min_index:\n",
    "            min_index = current_index\n",
    "            min_class = classes[i]\n",
    "\n",
    "    return min_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZutwVDJF_6u_"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cs_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m classify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m컴퓨터과학과\u001b[39m\u001b[38;5;124m'\u001b[39m, title)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m## '제목' 컬럼에 분류 함수 적용하여 분석 결과 저장\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m cs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_analysis\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcs_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m제목\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(classify_cs)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m## 분석 결과에서 최종 분류 클래스 추출하여 저장\u001b[39;00m\n\u001b[1;32m      8\u001b[0m cs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_analysis\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_class)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cs_df' is not defined"
     ]
    }
   ],
   "source": [
    "## 컴퓨터과학과 제목 분류 헬퍼 함수 정의\n",
    "def classify_cs(title):\n",
    "  return classify('컴퓨터과학과', title)\n",
    "\n",
    "## '제목' 컬럼에 분류 함수 적용하여 분석 결과 저장\n",
    "cs_df['class_analysis'] = cs_df['제목'].apply(classify_cs)\n",
    "## 분석 결과에서 최종 분류 클래스 추출하여 저장\n",
    "cs_df['class'] = cs_df['class_analysis'].apply(get_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXosbkzW_R4z"
   },
   "source": [
    "## 13-17 분류 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcNZ9FtU_8MY"
   },
   "outputs": [],
   "source": [
    "def plot_class(df):\n",
    "  ## 분류 클래스별 개수 계산 및 순서 재정렬\n",
    "  class_counts = df['class'].value_counts().reindex(['학사/전공', '학생활동', '외부정보'])\n",
    "\n",
    "  plt.figure(figsize=(8, 6)) ## 그래프 크기 설정\n",
    "  plt.pie(class_counts,\n",
    "          labels=class_counts.index,\n",
    "          autopct='%.1f%%', ## 퍼센트 표시 형식\n",
    "          startangle=90, ## 시작 각도 설정\n",
    "          )\n",
    "\n",
    "  plt.title('Class Distribution') ## 그래프 제목 설정\n",
    "  plt.axis('equal') ## 원형 비율 유지\n",
    "  plt.show() ## 그래프 표시\n",
    "\n",
    "plot_class(cs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mx4cTNzQAdFm"
   },
   "source": [
    "## 13-18 다른 학과의 분류 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHvvCO50_9kI"
   },
   "outputs": [],
   "source": [
    "def classify_ko(title):\n",
    "  return classify('국어국문학과', title)\n",
    "\n",
    "ko_df['class_analysis'] = ko_df['제목'].apply(classify_ko)\n",
    "ko_df['class'] = ko_df['class_analysis'].apply(get_class)\n",
    "\n",
    "def classify_law(title):\n",
    "  return classify('법학과', title)\n",
    "\n",
    "law_df['class_analysis'] = law_df['제목'].apply(classify_law)\n",
    "law_df['class'] = law_df['class_analysis'].apply(get_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DZHhHDG_--w"
   },
   "outputs": [],
   "source": [
    "plot_class(ko_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssLCuuP4AALo"
   },
   "outputs": [],
   "source": [
    "plot_class(law_df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "test_vLLM",
   "language": "python",
   "name": "test_vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
